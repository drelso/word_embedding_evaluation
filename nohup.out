
=================== MODEL PARAMETERS: =================== 

config_file:                     /home/diegor/word_embedding_evaluation/config.py
data_dir:                        /home/diegor/word_embedding_evaluation/data/
bnc_texts_dir:                   /home/diegor/data/British_National_Corpus/Texts/
bnc_data_dir:                    /home/diegor/data/British_National_Corpus/bnc_full_processed_data/
bnc_data:                        /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data.txt
use_data_subset:                 True
data_subset_size:                0.1
bnc_subset_data:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-1.txt
bnc_subset_tags:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-1_tags.txt
tokenised_data:                  /home/diegor/word_embedding_evaluation/data/tok_bnc_full_proc_data_shffl_sub-1.npy
counts_file:                     /home/diegor/word_embedding_evaluation/data/counts_bnc_full_proc_data_shffl_sub-1.csv
vocab_threshold:                 5
word2vec_embeds:                 /home/diegor/word_embedding_evaluation/data/embeds/word2vec_bnc_full_proc_data_voc_5.npy
hellingerPCA_embeds:             /home/diegor/word_embedding_evaluation/data/embeds/hellingerPCA_200d_bnc_full_proc_data_voc_5.npy
source_hellingerPCA_vocab:       /home/diegor/data/word_embeddings/HellingerPCA/vocab.txt
source_hellingerPCA_vecs:        /home/diegor/data/word_embeddings/HellingerPCA/words.txt
glove_embeds:                    /home/diegor/word_embedding_evaluation/data/embeds/glove_840B_300d_bnc_full_proc_data_voc_5.npy
source_glove_embeds:             /home/diegor/data/word_embeddings/GloVe/glove.840B.300d.txt

=================== / MODEL PARAMETERS: =================== 

Constructing vocabulary from counts file in /home/diegor/word_embedding_evaluation/data/counts_bnc_full_proc_data_shffl_sub-1.csv
45769 unique tokens in vocabulary with (with minimum frequency 5)
Word2Vec embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/word2vec_bnc_full_proc_data_voc_5.npy.
HellingerPCA embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/hellingerPCA_200d_bnc_full_proc_data_voc_5.npy.
No GloVe embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/glove_840B_300d_bnc_full_proc_data_voc_5.npy, creating an embeddings file.
Processed 0 lines, current word ,
Processed 100000 lines, current word apolitical
Processed 200000 lines, current word fkin
Processed 300000 lines, current word jeanie
Processed 400000 lines, current word Entwine
Processed 500000 lines, current word 1924-1925
Processed 600000 lines, current word Ski-doo
Processed 700000 lines, current word légumes
Processed 800000 lines, current word TSUBASA
Processed 900000 lines, current word eftda
Processed 1000000 lines, current word Crowd-Sourced
Processed 1100000 lines, current word Abnoraml
Processed 1200000 lines, current word NewsToob
Processed 1300000 lines, current word fuen
Processed 1400000 lines, current word Aragó
Processed 1500000 lines, current word OC-X
Processed 1600000 lines, current word syarat
Processed 1700000 lines, current word čia
Processed 1800000 lines, current word Mchedlishvili
Processed 1900000 lines, current word 00:29:24
Processed 2000000 lines, current word Blogger/Google
Processed 2100000 lines, current word ItPrice
Loaded the file, lines read, processing...
Num words in GloVe vocabulary: 2195872
/home/diegor/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
Index of "hello": 8088 
 hello 0.25233 0.10176 -0.67485 0.21117 0.43492 0.16542 0.48261 -0.81222 0.041321 0.78502 -0.077857 -0.66324 0.1464 -0.29289 -0.25488 0.019293 -0.20265 0.98232 0.028312 -0.081276 -0.1214 0.13126 -0.17648 0.13556 -0.16361 -0.22574 0.055006 -0.20308 0.20718 0.095785 0.22481 0.21537 -0.32982 -0.12241 -0.40031 -0.079381 -0.19958 -0.015083 -0.079139 -0.18132 0.20681 -0.36196 -0.30744 -0.24422 -0.23113 0.09798 0.1463 -0.062738 0.42934 -0.078038 -0.19627 0.65093 -0.22807 -0.30308 -0.12483 -0.17568 -0.14651 0.15361 -0.29518 0.15099 -0.51726 -0.033564 -0.23109 -0.7833 0.018029 -0.15719 0.02293 0.49639 0.029225 0.05669 0.14616 -0.19195 0.16244 0.23898 0.36431 0.45263 0.2456 0.23803 0.31399 0.3487 -0.035791 0.56108 -0.25345 0.051964 -0.10618 -0.30962 1.0585 -0.42025 0.18216 -0.11256 0.40576 0.11784 -0.19705 -0.075292 0.080723 -0.02782 -0.15617 -0.44681 -0.15165 0.1692 0.098255 -0.031894 0.087143 0.26082 0.002706 0.1319 0.34439 -0.37894 -0.4114 0.081571 -0.11674 -0.43711 0.011144 0.099353 0.26612 0.40025 0.18895 -0.18438 -0.30355 -0.2725 0.22468 -0.40614 0.15618 -0.16043 0.47147 0.0080203 0.56858 0.21934 -0.11181 0.79925 0.10714 -0.50146 0.063593 0.069465 0.15292 -0.2747 -0.20989 0.20737 -0.10681 0.40651 -2.6438 -0.31139 -0.32157 -0.26458 -0.35625 0.070013 -0.18838 0.48773 -0.26167 -0.020805 0.17819 0.15758 -0.13752 0.056464 0.30766 -0.066136 0.4748 -0.27335 0.09732 -0.20832 0.0039332 0.346 -0.08702 -0.54924 -0.18759 -0.17174 0.060324 -0.13521 0.10419 0.30165 0.05798 0.21872 -0.073594 -0.20423 -0.25279 -0.10471 -0.32163 0.12525 -0.31281 0.0097207 -0.26777 -0.61121 -0.11089 -0.13652 0.035135 -0.4939 0.084857 -0.15494 -0.063509 -0.23935 0.28272 0.10849 -0.3365 -0.60764 0.38576 -0.0095438 0.17499 -0.52723 0.62211 0.19544 -0.48977 0.036582 -0.128 -0.016827 0.25647 -0.31698 0.48257 -0.14184 0.11046 -0.3098 -0.63141 -0.37268 0.23183 -0.14268 -0.02341 0.022255 -0.044662 -0.16404 -0.25848 0.1629 0.024751 0.23348 0.27933 0.38998 -0.058968 0.11355 0.15673 0.18583 -0.19814 -0.48123 -0.035084 0.078458 -0.49833 0.10855 -0.20133 0.05292 -0.11583 -0.16009 0.16768 0.42362 -0.23106 0.082465 0.24296 -0.16786 0.0080409 0.085947 0.38033 0.072981 0.1633 0.24704 -0.11094 0.15115 -0.22068 -0.061944 -0.037091 -0.087923 -0.23181 0.15035 -0.19093 -0.19113 -0.11894 0.094908 -0.0043347 0.15362 -0.41201 -0.3073 0.18375 0.40206 -0.0034793 -0.10917 -0.69522 0.10161 -0.079256 0.40329 0.22285 -0.19374 -0.13315 0.073231 0.099832 0.11685 -0.21643 -0.1108 0.10341 0.097286 0.11196 -0.3894 -0.0089363 0.28809 -0.10792 0.028811 0.32545 0.26052 -0.038941 0.075204 0.46031 -0.06293 0.21661 0.17869 -0.51917 0.33591

Traceback (most recent call last):
  File "embed_eval_pipeline.py", line 47, in <module>
    glove_with_vocab(
  File "/home/diegor/word_embedding_evaluation/utils/word_rep_funcs.py", line 275, in glove_with_vocab
    embeddings[word] = [float(x) for x in word_vec[1:]]
  File "/home/diegor/word_embedding_evaluation/utils/word_rep_funcs.py", line 275, in <listcomp>
    embeddings[word] = [float(x) for x in word_vec[1:]]
ValueError: could not convert string to float: 'name@domain.com'

=================== MODEL PARAMETERS: =================== 

config_file:                     /home/diegor/word_embedding_evaluation/config.py
data_dir:                        /home/diegor/word_embedding_evaluation/data/
bnc_texts_dir:                   /home/diegor/data/British_National_Corpus/Texts/
bnc_data_dir:                    /home/diegor/data/British_National_Corpus/bnc_full_processed_data/
bnc_data:                        /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data.txt
use_data_subset:                 True
data_subset_size:                0.1
bnc_subset_data:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-1.txt
bnc_subset_tags:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-1_tags.txt
tokenised_data:                  /home/diegor/word_embedding_evaluation/data/tok_bnc_full_proc_data_shffl_sub-1.npy
counts_file:                     /home/diegor/word_embedding_evaluation/data/counts_bnc_full_proc_data_shffl_sub-1.csv
vocab_threshold:                 5
word2vec_embeds:                 /home/diegor/word_embedding_evaluation/data/embeds/word2vec_bnc_full_proc_data_voc_5.npy
hellingerPCA_embeds:             /home/diegor/word_embedding_evaluation/data/embeds/hellingerPCA_200d_bnc_full_proc_data_voc_5.npy
source_hellingerPCA_vocab:       /home/diegor/data/word_embeddings/HellingerPCA/vocab.txt
source_hellingerPCA_vecs:        /home/diegor/data/word_embeddings/HellingerPCA/words.txt
glove_embeds:                    /home/diegor/word_embedding_evaluation/data/embeds/glove_840B_300d_bnc_full_proc_data_voc_5.npy
source_glove_embeds:             /home/diegor/data/word_embeddings/GloVe/glove.840B.300d.txt

=================== / MODEL PARAMETERS: =================== 

Constructing vocabulary from counts file in /home/diegor/word_embedding_evaluation/data/counts_bnc_full_proc_data_shffl_sub-1.csv
45769 unique tokens in vocabulary with (with minimum frequency 5)
Word2Vec embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/word2vec_bnc_full_proc_data_voc_5.npy.
HellingerPCA embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/hellingerPCA_200d_bnc_full_proc_data_voc_5.npy.
No GloVe embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/glove_840B_300d_bnc_full_proc_data_voc_5.npy, creating an embeddings file.
Processed 0 lines, current word ,
Processed 100000 lines, current word apolitical
Processed 200000 lines, current word fkin
Processed 300000 lines, current word jeanie
Processed 400000 lines, current word Entwine
Processed 500000 lines, current word 1924-1925
Processed 600000 lines, current word Ski-doo
Processed 700000 lines, current word légumes
Processed 800000 lines, current word TSUBASA
Processed 900000 lines, current word eftda
Processed 1000000 lines, current word Crowd-Sourced
Processed 1100000 lines, current word Abnoraml
Processed 1200000 lines, current word NewsToob
Processed 1300000 lines, current word fuen
Processed 1400000 lines, current word Aragó
Processed 1500000 lines, current word OC-X
Processed 1600000 lines, current word syarat
Processed 1700000 lines, current word čia
Processed 1800000 lines, current word Mchedlishvili
Processed 1900000 lines, current word 00:29:24
Processed 2000000 lines, current word Blogger/Google
Processed 2100000 lines, current word ItPrice
Loaded the file, lines read, processing...
Num words in GloVe vocabulary: 2195872
/home/diegor/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
Index of "hello": 8088 
 hello 0.25233 0.10176 -0.67485 0.21117 0.43492 0.16542 0.48261 -0.81222 0.041321 0.78502 -0.077857 -0.66324 0.1464 -0.29289 -0.25488 0.019293 -0.20265 0.98232 0.028312 -0.081276 -0.1214 0.13126 -0.17648 0.13556 -0.16361 -0.22574 0.055006 -0.20308 0.20718 0.095785 0.22481 0.21537 -0.32982 -0.12241 -0.40031 -0.079381 -0.19958 -0.015083 -0.079139 -0.18132 0.20681 -0.36196 -0.30744 -0.24422 -0.23113 0.09798 0.1463 -0.062738 0.42934 -0.078038 -0.19627 0.65093 -0.22807 -0.30308 -0.12483 -0.17568 -0.14651 0.15361 -0.29518 0.15099 -0.51726 -0.033564 -0.23109 -0.7833 0.018029 -0.15719 0.02293 0.49639 0.029225 0.05669 0.14616 -0.19195 0.16244 0.23898 0.36431 0.45263 0.2456 0.23803 0.31399 0.3487 -0.035791 0.56108 -0.25345 0.051964 -0.10618 -0.30962 1.0585 -0.42025 0.18216 -0.11256 0.40576 0.11784 -0.19705 -0.075292 0.080723 -0.02782 -0.15617 -0.44681 -0.15165 0.1692 0.098255 -0.031894 0.087143 0.26082 0.002706 0.1319 0.34439 -0.37894 -0.4114 0.081571 -0.11674 -0.43711 0.011144 0.099353 0.26612 0.40025 0.18895 -0.18438 -0.30355 -0.2725 0.22468 -0.40614 0.15618 -0.16043 0.47147 0.0080203 0.56858 0.21934 -0.11181 0.79925 0.10714 -0.50146 0.063593 0.069465 0.15292 -0.2747 -0.20989 0.20737 -0.10681 0.40651 -2.6438 -0.31139 -0.32157 -0.26458 -0.35625 0.070013 -0.18838 0.48773 -0.26167 -0.020805 0.17819 0.15758 -0.13752 0.056464 0.30766 -0.066136 0.4748 -0.27335 0.09732 -0.20832 0.0039332 0.346 -0.08702 -0.54924 -0.18759 -0.17174 0.060324 -0.13521 0.10419 0.30165 0.05798 0.21872 -0.073594 -0.20423 -0.25279 -0.10471 -0.32163 0.12525 -0.31281 0.0097207 -0.26777 -0.61121 -0.11089 -0.13652 0.035135 -0.4939 0.084857 -0.15494 -0.063509 -0.23935 0.28272 0.10849 -0.3365 -0.60764 0.38576 -0.0095438 0.17499 -0.52723 0.62211 0.19544 -0.48977 0.036582 -0.128 -0.016827 0.25647 -0.31698 0.48257 -0.14184 0.11046 -0.3098 -0.63141 -0.37268 0.23183 -0.14268 -0.02341 0.022255 -0.044662 -0.16404 -0.25848 0.1629 0.024751 0.23348 0.27933 0.38998 -0.058968 0.11355 0.15673 0.18583 -0.19814 -0.48123 -0.035084 0.078458 -0.49833 0.10855 -0.20133 0.05292 -0.11583 -0.16009 0.16768 0.42362 -0.23106 0.082465 0.24296 -0.16786 0.0080409 0.085947 0.38033 0.072981 0.1633 0.24704 -0.11094 0.15115 -0.22068 -0.061944 -0.037091 -0.087923 -0.23181 0.15035 -0.19093 -0.19113 -0.11894 0.094908 -0.0043347 0.15362 -0.41201 -0.3073 0.18375 0.40206 -0.0034793 -0.10917 -0.69522 0.10161 -0.079256 0.40329 0.22285 -0.19374 -0.13315 0.073231 0.099832 0.11685 -0.21643 -0.1108 0.10341 0.097286 0.11196 -0.3894 -0.0089363 0.28809 -0.10792 0.028811 0.32545 0.26052 -0.038941 0.075204 0.46031 -0.06293 0.21661 0.17869 -0.51917 0.33591

Traceback (most recent call last):
  File "embed_eval_pipeline.py", line 47, in <module>
    glove_with_vocab(
  File "/home/diegor/word_embedding_evaluation/utils/word_rep_funcs.py", line 275, in glove_with_vocab
    raise ValueError(f'{word} does not match word in source: {word_vec[0]}')
ValueError: the does not match word in source: 0.27204

=================== MODEL PARAMETERS: =================== 

config_file:                     /home/diegor/word_embedding_evaluation/config.py
data_dir:                        /home/diegor/word_embedding_evaluation/data/
bnc_texts_dir:                   /home/diegor/data/British_National_Corpus/Texts/
bnc_data_dir:                    /home/diegor/data/British_National_Corpus/bnc_full_processed_data/
bnc_data:                        /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data.txt
use_data_subset:                 True
data_subset_size:                0.1
bnc_subset_data:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-1.txt
bnc_subset_tags:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-1_tags.txt
tokenised_data:                  /home/diegor/word_embedding_evaluation/data/tok_bnc_full_proc_data_shffl_sub-1.npy
counts_file:                     /home/diegor/word_embedding_evaluation/data/counts_bnc_full_proc_data_shffl_sub-1.csv
vocab_threshold:                 5
word2vec_embeds:                 /home/diegor/word_embedding_evaluation/data/embeds/word2vec_bnc_full_proc_data_voc_5.npy
hellingerPCA_embeds:             /home/diegor/word_embedding_evaluation/data/embeds/hellingerPCA_200d_bnc_full_proc_data_voc_5.npy
source_hellingerPCA_vocab:       /home/diegor/data/word_embeddings/HellingerPCA/vocab.txt
source_hellingerPCA_vecs:        /home/diegor/data/word_embeddings/HellingerPCA/words.txt
glove_embeds:                    /home/diegor/word_embedding_evaluation/data/embeds/glove_840B_300d_bnc_full_proc_data_voc_5.npy
source_glove_embeds:             /home/diegor/data/word_embeddings/GloVe/glove.840B.300d.txt

=================== / MODEL PARAMETERS: =================== 

Constructing vocabulary from counts file in /home/diegor/word_embedding_evaluation/data/counts_bnc_full_proc_data_shffl_sub-1.csv
45769 unique tokens in vocabulary with (with minimum frequency 5)
Word2Vec embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/word2vec_bnc_full_proc_data_voc_5.npy.
HellingerPCA embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/hellingerPCA_200d_bnc_full_proc_data_voc_5.npy.
No GloVe embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/glove_840B_300d_bnc_full_proc_data_voc_5.npy, creating an embeddings file.
Processed 0 lines, current word ,
Processed 100000 lines, current word apolitical
Processed 200000 lines, current word fkin
Processed 300000 lines, current word jeanie
Processed 400000 lines, current word Entwine
Processed 500000 lines, current word 1924-1925
Processed 600000 lines, current word Ski-doo
Processed 700000 lines, current word légumes
Processed 800000 lines, current word TSUBASA
Processed 900000 lines, current word eftda
Processed 1000000 lines, current word Crowd-Sourced
Processed 1100000 lines, current word Abnoraml
Processed 1200000 lines, current word NewsToob
Processed 1300000 lines, current word fuen
Processed 1400000 lines, current word Aragó
Processed 1500000 lines, current word OC-X
Processed 1600000 lines, current word syarat
Processed 1700000 lines, current word čia
Processed 1800000 lines, current word Mchedlishvili
Processed 1900000 lines, current word 00:29:24
Processed 2000000 lines, current word Blogger/Google
Processed 2100000 lines, current word ItPrice
Loaded the file, lines read, processing...
Num words in GloVe vocabulary: 2195872
/home/diegor/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
Index of "hello": 8088 
 hello 0.25233 0.10176 -0.67485 0.21117 0.43492 0.16542 0.48261 -0.81222 0.041321 0.78502 -0.077857 -0.66324 0.1464 -0.29289 -0.25488 0.019293 -0.20265 0.98232 0.028312 -0.081276 -0.1214 0.13126 -0.17648 0.13556 -0.16361 -0.22574 0.055006 -0.20308 0.20718 0.095785 0.22481 0.21537 -0.32982 -0.12241 -0.40031 -0.079381 -0.19958 -0.015083 -0.079139 -0.18132 0.20681 -0.36196 -0.30744 -0.24422 -0.23113 0.09798 0.1463 -0.062738 0.42934 -0.078038 -0.19627 0.65093 -0.22807 -0.30308 -0.12483 -0.17568 -0.14651 0.15361 -0.29518 0.15099 -0.51726 -0.033564 -0.23109 -0.7833 0.018029 -0.15719 0.02293 0.49639 0.029225 0.05669 0.14616 -0.19195 0.16244 0.23898 0.36431 0.45263 0.2456 0.23803 0.31399 0.3487 -0.035791 0.56108 -0.25345 0.051964 -0.10618 -0.30962 1.0585 -0.42025 0.18216 -0.11256 0.40576 0.11784 -0.19705 -0.075292 0.080723 -0.02782 -0.15617 -0.44681 -0.15165 0.1692 0.098255 -0.031894 0.087143 0.26082 0.002706 0.1319 0.34439 -0.37894 -0.4114 0.081571 -0.11674 -0.43711 0.011144 0.099353 0.26612 0.40025 0.18895 -0.18438 -0.30355 -0.2725 0.22468 -0.40614 0.15618 -0.16043 0.47147 0.0080203 0.56858 0.21934 -0.11181 0.79925 0.10714 -0.50146 0.063593 0.069465 0.15292 -0.2747 -0.20989 0.20737 -0.10681 0.40651 -2.6438 -0.31139 -0.32157 -0.26458 -0.35625 0.070013 -0.18838 0.48773 -0.26167 -0.020805 0.17819 0.15758 -0.13752 0.056464 0.30766 -0.066136 0.4748 -0.27335 0.09732 -0.20832 0.0039332 0.346 -0.08702 -0.54924 -0.18759 -0.17174 0.060324 -0.13521 0.10419 0.30165 0.05798 0.21872 -0.073594 -0.20423 -0.25279 -0.10471 -0.32163 0.12525 -0.31281 0.0097207 -0.26777 -0.61121 -0.11089 -0.13652 0.035135 -0.4939 0.084857 -0.15494 -0.063509 -0.23935 0.28272 0.10849 -0.3365 -0.60764 0.38576 -0.0095438 0.17499 -0.52723 0.62211 0.19544 -0.48977 0.036582 -0.128 -0.016827 0.25647 -0.31698 0.48257 -0.14184 0.11046 -0.3098 -0.63141 -0.37268 0.23183 -0.14268 -0.02341 0.022255 -0.044662 -0.16404 -0.25848 0.1629 0.024751 0.23348 0.27933 0.38998 -0.058968 0.11355 0.15673 0.18583 -0.19814 -0.48123 -0.035084 0.078458 -0.49833 0.10855 -0.20133 0.05292 -0.11583 -0.16009 0.16768 0.42362 -0.23106 0.082465 0.24296 -0.16786 0.0080409 0.085947 0.38033 0.072981 0.1633 0.24704 -0.11094 0.15115 -0.22068 -0.061944 -0.037091 -0.087923 -0.23181 0.15035 -0.19093 -0.19113 -0.11894 0.094908 -0.0043347 0.15362 -0.41201 -0.3073 0.18375 0.40206 -0.0034793 -0.10917 -0.69522 0.10161 -0.079256 0.40329 0.22285 -0.19374 -0.13315 0.073231 0.099832 0.11685 -0.21643 -0.1108 0.10341 0.097286 0.11196 -0.3894 -0.0089363 0.28809 -0.10792 0.028811 0.32545 0.26052 -0.038941 0.075204 0.46031 -0.06293 0.21661 0.17869 -0.51917 0.33591

Traceback (most recent call last):
  File "embed_eval_pipeline.py", line 47, in <module>
    glove_with_vocab(
  File "/home/diegor/word_embedding_evaluation/utils/word_rep_funcs.py", line 277, in glove_with_vocab
    embeddings[word] = [float(x) for x in word_vec]
  File "/home/diegor/word_embedding_evaluation/utils/word_rep_funcs.py", line 277, in <listcomp>
    embeddings[word] = [float(x) for x in word_vec]
ValueError: could not convert string to float: 'name@domain.com'

=================== MODEL PARAMETERS: =================== 

config_file:                     /home/diegor/word_embedding_evaluation/config.py
data_dir:                        /home/diegor/word_embedding_evaluation/data/
bnc_texts_dir:                   /home/diegor/data/British_National_Corpus/Texts/
bnc_data_dir:                    /home/diegor/data/British_National_Corpus/bnc_full_processed_data/
bnc_data:                        /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data.txt
use_data_subset:                 True
data_subset_size:                0.1
bnc_subset_data:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-1.txt
bnc_subset_tags:                 /home/diegor/data/British_National_Corpus/bnc_full_processed_data/bnc_full_proc_data_shffl_sub-1_tags.txt
tokenised_data:                  /home/diegor/word_embedding_evaluation/data/tok_bnc_full_proc_data_shffl_sub-1.npy
counts_file:                     /home/diegor/word_embedding_evaluation/data/counts_bnc_full_proc_data_shffl_sub-1.csv
vocab_threshold:                 5
word2vec_embeds:                 /home/diegor/word_embedding_evaluation/data/embeds/word2vec_bnc_full_proc_data_voc_5.npy
hellingerPCA_embeds:             /home/diegor/word_embedding_evaluation/data/embeds/hellingerPCA_200d_bnc_full_proc_data_voc_5.npy
source_hellingerPCA_vocab:       /home/diegor/data/word_embeddings/HellingerPCA/vocab.txt
source_hellingerPCA_vecs:        /home/diegor/data/word_embeddings/HellingerPCA/words.txt
glove_embeds:                    /home/diegor/word_embedding_evaluation/data/embeds/glove_840B_300d_bnc_full_proc_data_voc_5.npy
source_glove_embeds:             /home/diegor/data/word_embeddings/GloVe/glove.840B.300d.txt

=================== / MODEL PARAMETERS: =================== 

Constructing vocabulary from counts file in /home/diegor/word_embedding_evaluation/data/counts_bnc_full_proc_data_shffl_sub-1.csv
45769 unique tokens in vocabulary with (with minimum frequency 5)
Word2Vec embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/word2vec_bnc_full_proc_data_voc_5.npy.
HellingerPCA embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/hellingerPCA_200d_bnc_full_proc_data_voc_5.npy.
No GloVe embeddings file found at /home/diegor/word_embedding_evaluation/data/embeds/glove_840B_300d_bnc_full_proc_data_voc_5.npy, creating an embeddings file.
Processed 0 lines, current word ,
Processed 100000 lines, current word apolitical
Processed 200000 lines, current word fkin
Processed 300000 lines, current word jeanie
Processed 400000 lines, current word Entwine
Processed 500000 lines, current word 1924-1925
Processed 600000 lines, current word Ski-doo
Processed 700000 lines, current word légumes
Processed 800000 lines, current word TSUBASA
Processed 900000 lines, current word eftda
Processed 1000000 lines, current word Crowd-Sourced
Processed 1100000 lines, current word Abnoraml
Processed 1200000 lines, current word NewsToob
Processed 1300000 lines, current word fuen
Processed 1400000 lines, current word Aragó
Processed 1500000 lines, current word OC-X
Processed 1600000 lines, current word syarat
Processed 1700000 lines, current word čia
Processed 1800000 lines, current word Mchedlishvili
Processed 1900000 lines, current word 00:29:24
Processed 2000000 lines, current word Blogger/Google
Processed 2100000 lines, current word ItPrice
Loaded the file, lines read, processing...
Num words in GloVe vocabulary: 2195872
Index of "hello": 8088 
 hello 0.25233 0.10176 -0.67485 0.21117 0.43492 0.16542 0.48261 -0.81222 0.041321 0.78502 -0.077857 -0.66324 0.1464 -0.29289 -0.25488 0.019293 -0.20265 0.98232 0.028312 -0.081276 -0.1214 0.13126 -0.17648 0.13556 -0.16361 -0.22574 0.055006 -0.20308 0.20718 0.095785 0.22481 0.21537 -0.32982 -0.12241 -0.40031 -0.079381 -0.19958 -0.015083 -0.079139 -0.18132 0.20681 -0.36196 -0.30744 -0.24422 -0.23113 0.09798 0.1463 -0.062738 0.42934 -0.078038 -0.19627 0.65093 -0.22807 -0.30308 -0.12483 -0.17568 -0.14651 0.15361 -0.29518 0.15099 -0.51726 -0.033564 -0.23109 -0.7833 0.018029 -0.15719 0.02293 0.49639 0.029225 0.05669 0.14616 -0.19195 0.16244 0.23898 0.36431 0.45263 0.2456 0.23803 0.31399 0.3487 -0.035791 0.56108 -0.25345 0.051964 -0.10618 -0.30962 1.0585 -0.42025 0.18216 -0.11256 0.40576 0.11784 -0.19705 -0.075292 0.080723 -0.02782 -0.15617 -0.44681 -0.15165 0.1692 0.098255 -0.031894 0.087143 0.26082 0.002706 0.1319 0.34439 -0.37894 -0.4114 0.081571 -0.11674 -0.43711 0.011144 0.099353 0.26612 0.40025 0.18895 -0.18438 -0.30355 -0.2725 0.22468 -0.40614 0.15618 -0.16043 0.47147 0.0080203 0.56858 0.21934 -0.11181 0.79925 0.10714 -0.50146 0.063593 0.069465 0.15292 -0.2747 -0.20989 0.20737 -0.10681 0.40651 -2.6438 -0.31139 -0.32157 -0.26458 -0.35625 0.070013 -0.18838 0.48773 -0.26167 -0.020805 0.17819 0.15758 -0.13752 0.056464 0.30766 -0.066136 0.4748 -0.27335 0.09732 -0.20832 0.0039332 0.346 -0.08702 -0.54924 -0.18759 -0.17174 0.060324 -0.13521 0.10419 0.30165 0.05798 0.21872 -0.073594 -0.20423 -0.25279 -0.10471 -0.32163 0.12525 -0.31281 0.0097207 -0.26777 -0.61121 -0.11089 -0.13652 0.035135 -0.4939 0.084857 -0.15494 -0.063509 -0.23935 0.28272 0.10849 -0.3365 -0.60764 0.38576 -0.0095438 0.17499 -0.52723 0.62211 0.19544 -0.48977 0.036582 -0.128 -0.016827 0.25647 -0.31698 0.48257 -0.14184 0.11046 -0.3098 -0.63141 -0.37268 0.23183 -0.14268 -0.02341 0.022255 -0.044662 -0.16404 -0.25848 0.1629 0.024751 0.23348 0.27933 0.38998 -0.058968 0.11355 0.15673 0.18583 -0.19814 -0.48123 -0.035084 0.078458 -0.49833 0.10855 -0.20133 0.05292 -0.11583 -0.16009 0.16768 0.42362 -0.23106 0.082465 0.24296 -0.16786 0.0080409 0.085947 0.38033 0.072981 0.1633 0.24704 -0.11094 0.15115 -0.22068 -0.061944 -0.037091 -0.087923 -0.23181 0.15035 -0.19093 -0.19113 -0.11894 0.094908 -0.0043347 0.15362 -0.41201 -0.3073 0.18375 0.40206 -0.0034793 -0.10917 -0.69522 0.10161 -0.079256 0.40329 0.22285 -0.19374 -0.13315 0.073231 0.099832 0.11685 -0.21643 -0.1108 0.10341 0.097286 0.11196 -0.3894 -0.0089363 0.28809 -0.10792 0.028811 0.32545 0.26052 -0.038941 0.075204 0.46031 -0.06293 0.21661 0.17869 -0.51917 0.33591



 ['the', '0.27204', '-0.06203', '-0.1884', '0.023225', '-0.018158', '0.0067192', '-0.13877', '0.17708', '0.17709', '2.5882', '-0.35179', '-0.17312', '0.43285', '-0.10708', '0.15006', '-0.19982', '-0.19093', '1.1871', '-0.16207', '-0.23538', '0.003664', '-0.19156', '-0.085662', '0.039199', '-0.066449', '-0.04209', '-0.19122', '0.011679', '-0.37138', '0.21886', '0.0011423', '0.4319', '-0.14205', '0.38059', '0.30654', '0.020167', '-0.18316', '-0.0065186', '-0.0080549', '-0.12063', '0.027507', '0.29839', '-0.22896', '-0.22882', '0.14671', '-0.076301', '-0.1268', '-0.0066651', '-0.052795', '0.14258', '0.1561', '0.05551', '-0.16149', '0.09629', '-0.076533', '-0.049971', '-0.010195', '-0.047641', '-0.16679', '-0.2394', '0.0050141', '-0.049175', '0.013338', '0.41923', '-0.10104', '0.015111', '-0.077706', '-0.13471', '0.119', '0.10802', '0.21061', '-0.051904', '0.18527', '0.17856', '0.041293', '-0.014385', '-0.082567', '-0.035483', '-0.076173', '-0.045367', '0.089281', '0.33672', '-0.22099', '-0.0067275', '0.23983', '-0.23147', '-0.88592', '0.091297', '-0.012123', '0.013233', '-0.25799', '-0.02972', '0.016754', '0.01369', '0.32377', '0.039546', '0.042114', '-0.088243', '0.30318', '0.087747', '0.16346', '-0.40485', '-0.043845', '-0.040697', '0.20936', '-0.77795', '0.2997', '0.2334', '0.14891', '-0.39037', '-0.053086', '0.062922', '0.065663', '-0.13906', '0.094193', '0.10344', '-0.2797', '0.28905', '-0.32161', '0.020687', '0.063254', '-0.23257', '-0.4352', '-0.017049', '-0.32744', '-0.047064', '-0.075149', '-0.18788', '-0.015017', '0.029342', '-0.3527', '-0.044278', '-0.13507', '-0.11644', '-0.1043', '0.1392', '0.0039199', '0.37603', '0.067217', '-0.37992', '-1.1241', '-0.057357', '-0.16826', '0.03941', '0.2604', '-0.023866', '0.17963', '0.13553', '0.2139', '0.052633', '-0.25033', '-0.11307', '0.22234', '0.066597', '-0.11161', '0.062438', '-0.27972', '0.19878', '-0.36262', '-1.0006e-05', '-0.17262', '0.29166', '-0.15723', '0.054295', '0.06101', '-0.39165', '0.2766', '0.057816', '0.39709', '0.025229', '0.24672', '-0.08905', '0.15683', '-0.2096', '-0.22196', '0.052394', '-0.01136', '0.050417', '-0.14023', '-0.042825', '-0.031931', '-0.21336', '-0.20402', '-0.23272', '0.07449', '0.088202', '-0.11063', '-0.33526', '-0.014028', '-0.29429', '-0.086911', '-0.1321', '-0.43616', '0.20513', '0.0079362', '0.48505', '0.064237', '0.14261', '-0.43711', '0.12783', '-0.13111', '0.24673', '-0.27496', '0.15896', '0.43314', '0.090286', '0.24662', '0.066463', '-0.20099', '0.1101', '0.03644', '0.17359', '-0.15689', '-0.086328', '-0.17316', '0.36975', '-0.40317', '-0.064814', '-0.034166', '-0.013773', '0.062854', '-0.17183', '-0.12366', '-0.034663', '-0.22793', '-0.23172', '0.239', '0.27473', '0.15332', '0.10661', '-0.060982', '-0.024805', '-0.13478', '0.17932', '-0.37374', '-0.02893', '-0.11142', '-0.08389', '-0.055932', '0.068039', '-0.10783', '0.1465', '0.094617', '-0.084554', '0.067429', '-0.3291', '0.034082', '-0.16747', '-0.25997', '-0.22917', '0.020159', '-0.02758', '0.16136', '-0.18538', '0.037665', '0.57603', '0.20684', '0.27941', '0.16477', '-0.018769', '0.12062', '0.069648', '0.059022', '-0.23154', '0.24095', '-0.3471', '0.04854', '-0.056502', '0.41566', '-0.43194', '0.4823', '-0.051759', '-0.27285', '-0.25893', '0.16555', '-0.1831', '-0.06734', '0.42457', '0.010346', '0.14237', '0.25939', '0.17123', '-0.13821', '-0.066846', '0.015981', '-0.30193', '0.043579', '-0.043102', '0.35025', '-0.19681', '-0.4281', '0.16899', '0.22511', '-0.28557', '-0.1028', '-0.018168', '0.11407', '0.13015', '-0.18317', '0.1323']


 ['0.27204', '-0.06203', '-0.1884', '0.023225', '-0.018158', '0.0067192', '-0.13877', '0.17708', '0.17709', '2.5882', '-0.35179', '-0.17312', '0.43285', '-0.10708', '0.15006', '-0.19982', '-0.19093', '1.1871', '-0.16207', '-0.23538', '0.003664', '-0.19156', '-0.085662', '0.039199', '-0.066449', '-0.04209', '-0.19122', '0.011679', '-0.37138', '0.21886', '0.0011423', '0.4319', '-0.14205', '0.38059', '0.30654', '0.020167', '-0.18316', '-0.0065186', '-0.0080549', '-0.12063', '0.027507', '0.29839', '-0.22896', '-0.22882', '0.14671', '-0.076301', '-0.1268', '-0.0066651', '-0.052795', '0.14258', '0.1561', '0.05551', '-0.16149', '0.09629', '-0.076533', '-0.049971', '-0.010195', '-0.047641', '-0.16679', '-0.2394', '0.0050141', '-0.049175', '0.013338', '0.41923', '-0.10104', '0.015111', '-0.077706', '-0.13471', '0.119', '0.10802', '0.21061', '-0.051904', '0.18527', '0.17856', '0.041293', '-0.014385', '-0.082567', '-0.035483', '-0.076173', '-0.045367', '0.089281', '0.33672', '-0.22099', '-0.0067275', '0.23983', '-0.23147', '-0.88592', '0.091297', '-0.012123', '0.013233', '-0.25799', '-0.02972', '0.016754', '0.01369', '0.32377', '0.039546', '0.042114', '-0.088243', '0.30318', '0.087747', '0.16346', '-0.40485', '-0.043845', '-0.040697', '0.20936', '-0.77795', '0.2997', '0.2334', '0.14891', '-0.39037', '-0.053086', '0.062922', '0.065663', '-0.13906', '0.094193', '0.10344', '-0.2797', '0.28905', '-0.32161', '0.020687', '0.063254', '-0.23257', '-0.4352', '-0.017049', '-0.32744', '-0.047064', '-0.075149', '-0.18788', '-0.015017', '0.029342', '-0.3527', '-0.044278', '-0.13507', '-0.11644', '-0.1043', '0.1392', '0.0039199', '0.37603', '0.067217', '-0.37992', '-1.1241', '-0.057357', '-0.16826', '0.03941', '0.2604', '-0.023866', '0.17963', '0.13553', '0.2139', '0.052633', '-0.25033', '-0.11307', '0.22234', '0.066597', '-0.11161', '0.062438', '-0.27972', '0.19878', '-0.36262', '-1.0006e-05', '-0.17262', '0.29166', '-0.15723', '0.054295', '0.06101', '-0.39165', '0.2766', '0.057816', '0.39709', '0.025229', '0.24672', '-0.08905', '0.15683', '-0.2096', '-0.22196', '0.052394', '-0.01136', '0.050417', '-0.14023', '-0.042825', '-0.031931', '-0.21336', '-0.20402', '-0.23272', '0.07449', '0.088202', '-0.11063', '-0.33526', '-0.014028', '-0.29429', '-0.086911', '-0.1321', '-0.43616', '0.20513', '0.0079362', '0.48505', '0.064237', '0.14261', '-0.43711', '0.12783', '-0.13111', '0.24673', '-0.27496', '0.15896', '0.43314', '0.090286', '0.24662', '0.066463', '-0.20099', '0.1101', '0.03644', '0.17359', '-0.15689', '-0.086328', '-0.17316', '0.36975', '-0.40317', '-0.064814', '-0.034166', '-0.013773', '0.062854', '-0.17183', '-0.12366', '-0.034663', '-0.22793', '-0.23172', '0.239', '0.27473', '0.15332', '0.10661', '-0.060982', '-0.024805', '-0.13478', '0.17932', '-0.37374', '-0.02893', '-0.11142', '-0.08389', '-0.055932', '0.068039', '-0.10783', '0.1465', '0.094617', '-0.084554', '0.067429', '-0.3291', '0.034082', '-0.16747', '-0.25997', '-0.22917', '0.020159', '-0.02758', '0.16136', '-0.18538', '0.037665', '0.57603', '0.20684', '0.27941', '0.16477', '-0.018769', '0.12062', '0.069648', '0.059022', '-0.23154', '0.24095', '-0.3471', '0.04854', '-0.056502', '0.41566', '-0.43194', '0.4823', '-0.051759', '-0.27285', '-0.25893', '0.16555', '-0.1831', '-0.06734', '0.42457', '0.010346', '0.14237', '0.25939', '0.17123', '-0.13821', '-0.066846', '0.015981', '-0.30193', '0.043579', '-0.043102', '0.35025', '-0.19681', '-0.4281', '0.16899', '0.22511', '-0.28557', '-0.1028', '-0.018168', '0.11407', '0.13015', '-0.18317', '0.1323']


 the


 ['of', '0.060216', '0.21799', '-0.04249', '-0.38618', '-0.15388', '0.034635', '0.22243', '0.21718', '0.0068483', '2.4375', '-0.27418', '0.13572', '0.31086', '-0.063206', '0.00038225', '-0.18597', '-0.19333', '1.4447', '-0.38541', '-0.28549', '0.075627', '-0.036799', '-0.46068', '-0.016835', '0.19821', '-0.092746', '0.18954', '-0.00032648', '-0.17081', '0.50359', '0.46256', '0.26901', '-0.12256', '0.24713', '0.069305', '-0.20777', '-0.4456', '0.30223', '-0.0098344', '0.32772', '0.11038', '0.41271', '-0.15854', '-0.056983', '0.38918', '-0.21158', '-0.13307', '0.40406', '0.1749', '0.053949', '0.10984', '-0.18476', '-0.054014', '0.040112', '-0.10175', '0.12662', '0.069709', '-0.24071', '-0.20995', '-0.051381', '0.28219', '0.18598', '-0.5018', '0.27572', '-0.18497', '-0.18399', '0.15696', '-0.038444', '-0.52238', '0.22753', '0.048672', '-0.078837', '0.065448', '0.18399', '0.40211', '-0.12745', '-0.12302', '0.31072', '0.099588', '0.036047', '-0.25946', '0.36128', '0.12748', '-0.18667', '0.16502', '-0.3912', '-0.67549', '0.11291', '0.040743', '0.034973', '-0.04091', '-0.039791', '-0.40544', '-0.015867', '0.10239', '0.046868', '-0.082776', '0.015132', '-0.14899', '-0.25125', '0.25244', '-0.11851', '-0.34127', '0.016516', '0.30405', '-0.541', '0.305', '0.39065', '0.42362', '-0.41721', '-0.054247', '-0.26014', '-0.14048', '-0.14166', '-0.02105', '0.050822', '-0.078053', '0.45922', '0.17598', '-0.0157', '0.09118', '0.034263', '-0.49995', '0.028574', '0.12068', '0.19781', '-0.013025', '-0.22418', '0.12503', '0.14653', '-0.23085', '0.21987', '-0.059321', '-0.088169', '-0.1252', '0.0075112', '-0.22421', '0.6214', '0.2009', '-0.02899', '-0.65073', '0.0053506', '-0.12073', '0.20988', '-0.1684', '0.041826', '0.054582', '0.35247', '0.2006', '0.031903', '-0.053307', '-0.44009', '0.22495', '-0.30616', '-0.32855', '-0.015779', '-0.13913', '0.34309', '-0.13569', '-0.22276', '0.14295', '0.05501', '-0.10616', '0.23597', '-0.20701', '-0.30963', '0.13528', '-0.16144', '0.29108', '0.12301', '0.2365', '-0.26153', '0.31022', '0.20612', '-0.19885', '0.10971', '-0.0018054', '0.14621', '0.15177', '-0.4468', '0.0067433', '-0.028784', '0.13821', '-0.16566', '-0.45517', '0.016623', '0.10703', '-0.48399', '0.040033', '0.049625', '-0.26454', '-0.1468', '0.13651', '0.15261', '0.067522', '0.50405', '-0.18848', '0.15256', '-0.26997', '0.055578', '0.047077', '-0.17848', '-0.33567', '-0.03148', '0.19107', '0.18818', '0.18778', '0.18313', '-0.364', '-0.0054127', '-0.15763', '0.16386', '-0.084828', '-0.19838', '-0.40454', '0.41031', '-0.41393', '0.029771', '0.10544', '-0.11295', '-0.068076', '-0.22372', '-0.19084', '-0.080269', '-0.38345', '0.064712', '0.23111', '0.21408', '0.28038', '0.14221', '-0.20696', '0.015874', '-0.14112', '0.089859', '-0.21533', '-0.020105', '0.22703', '0.083425', '-0.2958', '0.018036', '0.19885', '0.17794', '0.13688', '-0.10302', '0.029651', '0.051271', '-0.14787', '-0.41824', '0.019828', '-0.26385', '-0.074654', '-0.015718', '0.48094', '0.12492', '-0.11409', '0.58127', '0.095836', '-0.095912', '-0.057435', '0.13883', '0.10307', '0.081362', '-0.4669', '0.50705', '0.021685', '-0.071623', '-0.063827', '-0.11154', '0.61792', '-0.56329', '0.023565', '0.18041', '-0.2578', '-0.50956', '0.14737', '-0.033317', '-0.037053', '0.24062', '0.12641', '-0.027091', '0.4039', '-0.02836', '-0.022235', '-0.11493', '-0.2285', '-0.05746', '0.2952', '-0.21914', '-0.13307', '-0.23647', '-0.42484', '0.11606', '0.0048131', '-0.39629', '-0.26823', '0.3292', '-0.17597', '0.11709', '-0.16692', '-0.094085']


 ['0.060216', '0.21799', '-0.04249', '-0.38618', '-0.15388', '0.034635', '0.22243', '0.21718', '0.0068483', '2.4375', '-0.27418', '0.13572', '0.31086', '-0.063206', '0.00038225', '-0.18597', '-0.19333', '1.4447', '-0.38541', '-0.28549', '0.075627', '-0.036799', '-0.46068', '-0.016835', '0.19821', '-0.092746', '0.18954', '-0.00032648', '-0.17081', '0.50359', '0.46256', '0.26901', '-0.12256', '0.24713', '0.069305', '-0.20777', '-0.4456', '0.30223', '-0.0098344', '0.32772', '0.11038', '0.41271', '-0.15854', '-0.056983', '0.38918', '-0.21158', '-0.13307', '0.40406', '0.1749', '0.053949', '0.10984', '-0.18476', '-0.054014', '0.040112', '-0.10175', '0.12662', '0.069709', '-0.24071', '-0.20995', '-0.051381', '0.28219', '0.18598', '-0.5018', '0.27572', '-0.18497', '-0.18399', '0.15696', '-0.038444', '-0.52238', '0.22753', '0.048672', '-0.078837', '0.065448', '0.18399', '0.40211', '-0.12745', '-0.12302', '0.31072', '0.099588', '0.036047', '-0.25946', '0.36128', '0.12748', '-0.18667', '0.16502', '-0.3912', '-0.67549', '0.11291', '0.040743', '0.034973', '-0.04091', '-0.039791', '-0.40544', '-0.015867', '0.10239', '0.046868', '-0.082776', '0.015132', '-0.14899', '-0.25125', '0.25244', '-0.11851', '-0.34127', '0.016516', '0.30405', '-0.541', '0.305', '0.39065', '0.42362', '-0.41721', '-0.054247', '-0.26014', '-0.14048', '-0.14166', '-0.02105', '0.050822', '-0.078053', '0.45922', '0.17598', '-0.0157', '0.09118', '0.034263', '-0.49995', '0.028574', '0.12068', '0.19781', '-0.013025', '-0.22418', '0.12503', '0.14653', '-0.23085', '0.21987', '-0.059321', '-0.088169', '-0.1252', '0.0075112', '-0.22421', '0.6214', '0.2009', '-0.02899', '-0.65073', '0.0053506', '-0.12073', '0.20988', '-0.1684', '0.041826', '0.054582', '0.35247', '0.2006', '0.031903', '-0.053307', '-0.44009', '0.22495', '-0.30616', '-0.32855', '-0.015779', '-0.13913', '0.34309', '-0.13569', '-0.22276', '0.14295', '0.05501', '-0.10616', '0.23597', '-0.20701', '-0.30963', '0.13528', '-0.16144', '0.29108', '0.12301', '0.2365', '-0.26153', '0.31022', '0.20612', '-0.19885', '0.10971', '-0.0018054', '0.14621', '0.15177', '-0.4468', '0.0067433', '-0.028784', '0.13821', '-0.16566', '-0.45517', '0.016623', '0.10703', '-0.48399', '0.040033', '0.049625', '-0.26454', '-0.1468', '0.13651', '0.15261', '0.067522', '0.50405', '-0.18848', '0.15256', '-0.26997', '0.055578', '0.047077', '-0.17848', '-0.33567', '-0.03148', '0.19107', '0.18818', '0.18778', '0.18313', '-0.364', '-0.0054127', '-0.15763', '0.16386', '-0.084828', '-0.19838', '-0.40454', '0.41031', '-0.41393', '0.029771', '0.10544', '-0.11295', '-0.068076', '-0.22372', '-0.19084', '-0.080269', '-0.38345', '0.064712', '0.23111', '0.21408', '0.28038', '0.14221', '-0.20696', '0.015874', '-0.14112', '0.089859', '-0.21533', '-0.020105', '0.22703', '0.083425', '-0.2958', '0.018036', '0.19885', '0.17794', '0.13688', '-0.10302', '0.029651', '0.051271', '-0.14787', '-0.41824', '0.019828', '-0.26385', '-0.074654', '-0.015718', '0.48094', '0.12492', '-0.11409', '0.58127', '0.095836', '-0.095912', '-0.057435', '0.13883', '0.10307', '0.081362', '-0.4669', '0.50705', '0.021685', '-0.071623', '-0.063827', '-0.11154', '0.61792', '-0.56329', '0.023565', '0.18041', '-0.2578', '-0.50956', '0.14737', '-0.033317', '-0.037053', '0.24062', '0.12641', '-0.027091', '0.4039', '-0.02836', '-0.022235', '-0.11493', '-0.2285', '-0.05746', '0.2952', '-0.21914', '-0.13307', '-0.23647', '-0.42484', '0.11606', '0.0048131', '-0.39629', '-0.26823', '0.3292', '-0.17597', '0.11709', '-0.16692', '-0.094085']


 of


 ['and', '-0.18567', '0.066008', '-0.25209', '-0.11725', '0.26513', '0.064908', '0.12291', '-0.093979', '0.024321', '2.4926', '-0.017916', '-0.071218', '-0.24782', '-0.26237', '-0.2246', '-0.21961', '-0.12927', '1.0867', '-0.66072', '-0.031617', '-0.057328', '0.056903', '-0.27939', '-0.39825', '0.14251', '-0.085146', '-0.14779', '0.055067', '-0.0028687', '-0.20917', '-0.070735', '0.22577', '-0.15881', '-0.10395', '0.09711', '-0.56251', '-0.32929', '-0.20853', '0.0098711', '0.049777', '0.0014883', '0.15884', '0.042771', '-0.0026956', '-0.02462', '-0.19213', '-0.22556', '0.10838', '0.090086', '-0.13291', '0.32559', '-0.17038', '-0.1099', '-0.23986', '-0.024289', '0.014656', '-0.237', '0.084828', '-0.35982', '-0.076746', '0.048909', '0.11431', '-0.21013', '0.24765', '-0.017531', '-0.14028', '0.046191', '0.22972', '0.1175', '0.12724', '0.012992', '0.4587', '0.41085', '0.039106', '0.15713', '-0.18376', '0.26834', '0.056662', '0.16844', '-0.053788', '-0.091892', '0.11193', '-0.08681', '-0.13324', '0.15062', '-0.31733', '-0.22078', '0.25038', '0.34131', '0.36419', '-0.089514', '-0.22193', '0.24471', '0.040091', '0.47798', '-0.029996', '0.0019212', '0.063511', '-0.20417', '-0.26478', '0.20649', '0.015573', '-0.27722', '-0.18861', '-0.10289', '-0.49773', '0.14986', '-0.010877', '0.25085', '-0.28117', '0.18966', '-0.065879', '0.094753', '-0.15338', '-0.055071', '-0.36747', '0.24993', '0.096527', '0.23538', '0.18405', '0.052859', '0.22967', '0.12582', '0.15536', '-0.17275', '0.33946', '-0.10049', '0.074948', '-0.093575', '-0.04049', '-0.016922', '-0.0058039', '-0.18108', '0.19537', '0.45178', '0.10965', '0.2337', '-0.09905', '-0.078633', '0.21678', '-0.71231', '-0.099759', '0.33333', '-0.1646', '-0.091688', '0.21056', '0.023669', '0.028922', '0.1199', '-0.12512', '-0.026037', '-0.062217', '0.55816', '0.0050273', '-0.30888', '0.038611', '0.17568', '-0.11163', '-0.10815', '-0.19444', '0.29433', '0.14519', '-0.042878', '0.18534', '0.018891', '-0.61883', '0.13352', '0.036007', '0.33995', '0.22109', '-0.079328', '0.071319', '0.17678', '0.16378', '-0.23142', '-0.1434', '-0.098122', '-0.019286', '0.2356', '-0.34013', '-0.061007', '-0.23208', '-0.31152', '0.10063', '-0.15957', '0.20183', '-0.016345', '-0.12303', '0.022667', '-0.20986', '-0.20127', '-0.087883', '0.064731', '0.10195', '-0.1786', '0.33056', '0.21407', '-0.32165', '-0.17106', '0.19407', '-0.38618', '-0.2148', '-0.052254', '0.023175', '0.47389', '0.18612', '0.12711', '0.20855', '-0.10256', '-0.12016', '-0.40488', '0.029695', '-0.027419', '-0.0085227', '-0.11415', '0.081134', '-0.17228', '0.19142', '0.026514', '0.043789', '-0.12399', '0.13354', '0.10112', '0.081682', '-0.15085', '0.0075806', '-0.18971', '0.24669', '0.22491', '0.35553', '-0.3277', '-0.21821', '0.1402', '0.28604', '0.055226', '-0.086544', '0.02111', '-0.19236', '0.074245', '0.076782', '0.00081666', '0.034097', '-0.57719', '0.10657', '0.28134', '-0.11964', '-0.68281', '-0.32893', '-0.24442', '-0.025847', '0.0091273', '0.2025', '-0.050959', '-0.11042', '0.010962', '0.076773', '0.40048', '-0.40739', '-0.44773', '0.31954', '-0.036326', '-0.012789', '-0.17282', '0.1476', '0.2356', '0.080642', '-0.36528', '-0.0083443', '0.6239', '-0.24379', '0.019917', '-0.28803', '-0.010494', '0.038412', '-0.11718', '-0.072462', '0.16381', '0.38488', '-0.029783', '0.23444', '0.4532', '0.14815', '-0.027021', '-0.073181', '-0.1147', '-0.0054545', '0.47796', '0.090912', '0.094489', '-0.36882', '-0.59396', '-0.097729', '0.20072', '0.17055', '-0.0047356', '-0.039709', '0.32498', '-0.023452', '0.12302', '0.3312']


 ['-0.18567', '0.066008', '-0.25209', '-0.11725', '0.26513', '0.064908', '0.12291', '-0.093979', '0.024321', '2.4926', '-0.017916', '-0.071218', '-0.24782', '-0.26237', '-0.2246', '-0.21961', '-0.12927', '1.0867', '-0.66072', '-0.031617', '-0.057328', '0.056903', '-0.27939', '-0.39825', '0.14251', '-0.085146', '-0.14779', '0.055067', '-0.0028687', '-0.20917', '-0.070735', '0.22577', '-0.15881', '-0.10395', '0.09711', '-0.56251', '-0.32929', '-0.20853', '0.0098711', '0.049777', '0.0014883', '0.15884', '0.042771', '-0.0026956', '-0.02462', '-0.19213', '-0.22556', '0.10838', '0.090086', '-0.13291', '0.32559', '-0.17038', '-0.1099', '-0.23986', '-0.024289', '0.014656', '-0.237', '0.084828', '-0.35982', '-0.076746', '0.048909', '0.11431', '-0.21013', '0.24765', '-0.017531', '-0.14028', '0.046191', '0.22972', '0.1175', '0.12724', '0.012992', '0.4587', '0.41085', '0.039106', '0.15713', '-0.18376', '0.26834', '0.056662', '0.16844', '-0.053788', '-0.091892', '0.11193', '-0.08681', '-0.13324', '0.15062', '-0.31733', '-0.22078', '0.25038', '0.34131', '0.36419', '-0.089514', '-0.22193', '0.24471', '0.040091', '0.47798', '-0.029996', '0.0019212', '0.063511', '-0.20417', '-0.26478', '0.20649', '0.015573', '-0.27722', '-0.18861', '-0.10289', '-0.49773', '0.14986', '-0.010877', '0.25085', '-0.28117', '0.18966', '-0.065879', '0.094753', '-0.15338', '-0.055071', '-0.36747', '0.24993', '0.096527', '0.23538', '0.18405', '0.052859', '0.22967', '0.12582', '0.15536', '-0.17275', '0.33946', '-0.10049', '0.074948', '-0.093575', '-0.04049', '-0.016922', '-0.0058039', '-0.18108', '0.19537', '0.45178', '0.10965', '0.2337', '-0.09905', '-0.078633', '0.21678', '-0.71231', '-0.099759', '0.33333', '-0.1646', '-0.091688', '0.21056', '0.023669', '0.028922', '0.1199', '-0.12512', '-0.026037', '-0.062217', '0.55816', '0.0050273', '-0.30888', '0.038611', '0.17568', '-0.11163', '-0.10815', '-0.19444', '0.29433', '0.14519', '-0.042878', '0.18534', '0.018891', '-0.61883', '0.13352', '0.036007', '0.33995', '0.22109', '-0.079328', '0.071319', '0.17678', '0.16378', '-0.23142', '-0.1434', '-0.098122', '-0.019286', '0.2356', '-0.34013', '-0.061007', '-0.23208', '-0.31152', '0.10063', '-0.15957', '0.20183', '-0.016345', '-0.12303', '0.022667', '-0.20986', '-0.20127', '-0.087883', '0.064731', '0.10195', '-0.1786', '0.33056', '0.21407', '-0.32165', '-0.17106', '0.19407', '-0.38618', '-0.2148', '-0.052254', '0.023175', '0.47389', '0.18612', '0.12711', '0.20855', '-0.10256', '-0.12016', '-0.40488', '0.029695', '-0.027419', '-0.0085227', '-0.11415', '0.081134', '-0.17228', '0.19142', '0.026514', '0.043789', '-0.12399', '0.13354', '0.10112', '0.081682', '-0.15085', '0.0075806', '-0.18971', '0.24669', '0.22491', '0.35553', '-0.3277', '-0.21821', '0.1402', '0.28604', '0.055226', '-0.086544', '0.02111', '-0.19236', '0.074245', '0.076782', '0.00081666', '0.034097', '-0.57719', '0.10657', '0.28134', '-0.11964', '-0.68281', '-0.32893', '-0.24442', '-0.025847', '0.0091273', '0.2025', '-0.050959', '-0.11042', '0.010962', '0.076773', '0.40048', '-0.40739', '-0.44773', '0.31954', '-0.036326', '-0.012789', '-0.17282', '0.1476', '0.2356', '0.080642', '-0.36528', '-0.0083443', '0.6239', '-0.24379', '0.019917', '-0.28803', '-0.010494', '0.038412', '-0.11718', '-0.072462', '0.16381', '0.38488', '-0.029783', '0.23444', '0.4532', '0.14815', '-0.027021', '-0.073181', '-0.1147', '-0.0054545', '0.47796', '0.090912', '0.094489', '-0.36882', '-0.59396', '-0.097729', '0.20072', '0.17055', '-0.0047356', '-0.039709', '0.32498', '-0.023452', '0.12302', '0.3312']


 and


 ['to', 'name@domain.com', '0.33865', '0.12698', '-0.16885', '0.55476', '0.48296', '0.45018', '0.0094233', '-0.36575', '-0.87561', '-0.35802', '0.2379', '0.31284', '-0.081367', '0.061482', '0.81921', '0.77488', '0.68518', '-0.48005', '-0.012098', '0.53366', '0.038321', '0.26857', '0.56736', '0.20427', '0.2847', '0.68113', '-0.26921', '0.10099', '-0.33252', '-0.22999', '0.66003', '0.21833', '-0.086523', '-0.30044', '-0.42253', '0.47525', '-0.2165', '0.3268', '0.63515', '-0.15657', '-0.25835', '-0.11663', '-0.41092', '-0.73779', '-0.0015122', '0.14481', '0.13287', '0.26097', '0.58175', '0.29285', '0.27168', '-0.0058512', '0.27731', '-0.40565', '0.05047', '0.059203', '-0.39081', '-0.098029', '-0.13969', '0.42714', '-0.20103', '-0.019703', '1.1957', '0.36278', '-0.6468', '0.096856', '-0.54383', '0.29666', '-0.0098302', '0.5042', '-0.3419', '-0.25067', '0.72375', '0.81957', '0.57959', '-0.28619', '0.91511', '-0.49127', '0.42129', '0.11429', '-0.32411', '-0.092257', '-1.0342', '-0.25774', '-0.19044', '-0.34201', '1.2339', '-0.65392', '0.83096', '-0.3133', '-0.10256', '-0.075641', '0.88435', '-0.1656', '0.041536', '0.23504', '-0.59116', '-0.12573', '0.48002', '0.23561', '-0.16167', '-0.1404', '-0.45722', '-0.13186', '0.9173', '-0.78831', '-0.027372', '0.036396', '-0.49337', '-0.2556', '-0.15139', '-0.53403', '0.90196', '0.60318', '0.43602', '-0.075461', '-0.09196', '0.14248', '0.14335', '0.67287', '0.52725', '-0.21312', '-0.31636', '0.63391', '0.33626', '-0.037607', '0.60729', '0.46028', '0.0010946', '0.22783', '0.38501', '-0.85127', '-0.76092', '0.20159', '-0.13641', '-0.15588', '0.060973', '0.13442', '-0.25715', '0.054989', '0.82028', '0.32086', '-0.94738', '-0.55948', '-0.072619', '-0.31286', '0.53855', '-0.27876', '-0.45292', '1.0417', '0.59105', '0.10445', '-0.0042194', '-0.11733', '0.38836', '0.43267', '0.43228', '-0.3419', '0.091655', '0.47851', '0.62238', '0.091656', '-0.11656', '-0.6118', '0.076486', '-0.27552', '-0.33407', '0.30166', '-0.089347', '-0.4676', '0.14198', '0.2924', '0.69944', '-0.53839', '0.14589', '0.36273', '-0.18743', '-0.32278', '-0.46289', '-0.0079593', '0.025591', '0.47468', '0.79237', '-0.72242', '0.92688', '0.49471', '-0.39816', '-0.099986', '0.15334', '-0.050664', '0.74726', '-0.047998', '0.43063', '-0.1613', '-0.020673', '0.98916', '-0.27938', '0.10797', '0.55202', '0.37686', '0.17607', '-0.27058', '-0.1362', '-0.024761', '0.63829', '-0.0054866', '0.12085', '0.56029', '0.28192', '0.29992', '0.23869', '0.070296', '0.34272', '-0.11676', '-0.24377', '0.90668', '0.1251', '-0.12149', '-0.057172', '0.36016', '0.057005', '-0.33869', '0.14762', '0.91449', '-0.20497', '-0.64492', '-0.47259', '-0.43348', '-0.47966', '-0.84446', '-0.35207', '0.0050338', '-0.33834', '0.15072', '-0.9816', '0.13519', '-0.33266', '0.11602', '0.032961', '-0.58063', '0.039186', '-0.34515', '-0.088268', '-0.49642', '0.56537', '-0.13342', '0.29861', '0.4289', '0.72562', '-0.61233', '-0.41913', '0.19969', '-0.64272', '-0.62544', '0.58792', '-0.33489', '-0.066748', '-0.42025', '-0.25105', '-0.52392', '-0.17985', '-0.059297', '-0.28532', '-0.76441', '0.41743', '0.83472', '0.46515', '0.3741', '0.35699', '-0.11646', '0.10322', '-0.16215', '-0.30452', '-0.0082364', '-0.49549', '0.47146', '-0.054368', '-0.18286', '0.083092', '-0.2659', '0.449', '0.3066', '0.19909', '0.21744', '0.77927', '-0.09052', '-0.28405', '0.02556', '-0.27124', '-0.26324', '0.45414', '0.19379', '-0.21259', '-0.38467', '0.18494', '-0.26692', '-0.18104', '0.051336', '-0.25989', '-0.15024']


 ['name@domain.com', '0.33865', '0.12698', '-0.16885', '0.55476', '0.48296', '0.45018', '0.0094233', '-0.36575', '-0.87561', '-0.35802', '0.2379', '0.31284', '-0.081367', '0.061482', '0.81921', '0.77488', '0.68518', '-0.48005', '-0.012098', '0.53366', '0.038321', '0.26857', '0.56736', '0.20427', '0.2847', '0.68113', '-0.26921', '0.10099', '-0.33252', '-0.22999', '0.66003', '0.21833', '-0.086523', '-0.30044', '-0.42253', '0.47525', '-0.2165', '0.3268', '0.63515', '-0.15657', '-0.25835', '-0.11663', '-0.41092', '-0.73779', '-0.0015122', '0.14481', '0.13287', '0.26097', '0.58175', '0.29285', '0.27168', '-0.0058512', '0.27731', '-0.40565', '0.05047', '0.059203', '-0.39081', '-0.098029', '-0.13969', '0.42714', '-0.20103', '-0.019703', '1.1957', '0.36278', '-0.6468', '0.096856', '-0.54383', '0.29666', '-0.0098302', '0.5042', '-0.3419', '-0.25067', '0.72375', '0.81957', '0.57959', '-0.28619', '0.91511', '-0.49127', '0.42129', '0.11429', '-0.32411', '-0.092257', '-1.0342', '-0.25774', '-0.19044', '-0.34201', '1.2339', '-0.65392', '0.83096', '-0.3133', '-0.10256', '-0.075641', '0.88435', '-0.1656', '0.041536', '0.23504', '-0.59116', '-0.12573', '0.48002', '0.23561', '-0.16167', '-0.1404', '-0.45722', '-0.13186', '0.9173', '-0.78831', '-0.027372', '0.036396', '-0.49337', '-0.2556', '-0.15139', '-0.53403', '0.90196', '0.60318', '0.43602', '-0.075461', '-0.09196', '0.14248', '0.14335', '0.67287', '0.52725', '-0.21312', '-0.31636', '0.63391', '0.33626', '-0.037607', '0.60729', '0.46028', '0.0010946', '0.22783', '0.38501', '-0.85127', '-0.76092', '0.20159', '-0.13641', '-0.15588', '0.060973', '0.13442', '-0.25715', '0.054989', '0.82028', '0.32086', '-0.94738', '-0.55948', '-0.072619', '-0.31286', '0.53855', '-0.27876', '-0.45292', '1.0417', '0.59105', '0.10445', '-0.0042194', '-0.11733', '0.38836', '0.43267', '0.43228', '-0.3419', '0.091655', '0.47851', '0.62238', '0.091656', '-0.11656', '-0.6118', '0.076486', '-0.27552', '-0.33407', '0.30166', '-0.089347', '-0.4676', '0.14198', '0.2924', '0.69944', '-0.53839', '0.14589', '0.36273', '-0.18743', '-0.32278', '-0.46289', '-0.0079593', '0.025591', '0.47468', '0.79237', '-0.72242', '0.92688', '0.49471', '-0.39816', '-0.099986', '0.15334', '-0.050664', '0.74726', '-0.047998', '0.43063', '-0.1613', '-0.020673', '0.98916', '-0.27938', '0.10797', '0.55202', '0.37686', '0.17607', '-0.27058', '-0.1362', '-0.024761', '0.63829', '-0.0054866', '0.12085', '0.56029', '0.28192', '0.29992', '0.23869', '0.070296', '0.34272', '-0.11676', '-0.24377', '0.90668', '0.1251', '-0.12149', '-0.057172', '0.36016', '0.057005', '-0.33869', '0.14762', '0.91449', '-0.20497', '-0.64492', '-0.47259', '-0.43348', '-0.47966', '-0.84446', '-0.35207', '0.0050338', '-0.33834', '0.15072', '-0.9816', '0.13519', '-0.33266', '0.11602', '0.032961', '-0.58063', '0.039186', '-0.34515', '-0.088268', '-0.49642', '0.56537', '-0.13342', '0.29861', '0.4289', '0.72562', '-0.61233', '-0.41913', '0.19969', '-0.64272', '-0.62544', '0.58792', '-0.33489', '-0.066748', '-0.42025', '-0.25105', '-0.52392', '-0.17985', '-0.059297', '-0.28532', '-0.76441', '0.41743', '0.83472', '0.46515', '0.3741', '0.35699', '-0.11646', '0.10322', '-0.16215', '-0.30452', '-0.0082364', '-0.49549', '0.47146', '-0.054368', '-0.18286', '0.083092', '-0.2659', '0.449', '0.3066', '0.19909', '0.21744', '0.77927', '-0.09052', '-0.28405', '0.02556', '-0.27124', '-0.26324', '0.45414', '0.19379', '-0.21259', '-0.38467', '0.18494', '-0.26692', '-0.18104', '0.051336', '-0.25989', '-0.15024']/home/diegor/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)



 to
Traceback (most recent call last):
  File "embed_eval_pipeline.py", line 47, in <module>
    glove_with_vocab(
  File "/home/diegor/word_embedding_evaluation/utils/word_rep_funcs.py", line 281, in glove_with_vocab
    embeddings[word] = [float(x) for x in word_vec]
  File "/home/diegor/word_embedding_evaluation/utils/word_rep_funcs.py", line 281, in <listcomp>
    embeddings[word] = [float(x) for x in word_vec]
ValueError: could not convert string to float: 'name@domain.com'
